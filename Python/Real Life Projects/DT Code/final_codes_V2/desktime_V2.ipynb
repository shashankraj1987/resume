{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Imports\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress Bar\n",
    "\n",
    "\n",
    "def progress_bar(progress, total, obj):\n",
    "    percent = 100*(float(progress)/float(total))\n",
    "    bar = 'â–ˆ'*int(percent)+'-'*(100-int(percent))\n",
    "    print(f\"\\r|{bar}| {percent: .2f}% Processing : {obj}\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Get API Keys and Connection.\n",
    "\n",
    "\n",
    "def read_creds(cred_loc, main=1, backup=0):\n",
    "    if os.path.exists(cred_loc):\n",
    "        f = open(cred_loc)\n",
    "        creds = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "        api_key = creds[\"nagmani.b@exalogic.co\"]\n",
    "\n",
    "        emp_url = creds[\"emp_url\"]\n",
    "        app_url = creds[\"app_url\"]\n",
    "        print(\"Received the Credentials.\")\n",
    "        return api_key, emp_url, app_url\n",
    "    else:\n",
    "        print(\"File not found\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract: Extract the Data from Desktime app.\n",
    "\n",
    "\n",
    "def get_dt_data(url: str, api: str, dt: str, emp=\"\"):\n",
    "    if emp:\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                url, params={\"apiKey\": api, \"date\": dt, \"id\": emp})\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return \"\"\n",
    "        else:\n",
    "            # print(\"Desktime Connection Successful.\")\n",
    "            return r.json()\n",
    "    else:\n",
    "        try:\n",
    "            # print(\"Desktime Connection Successful.\")\n",
    "            r = requests.get(url, params={\"apiKey\": api, \"date\": dt})\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return \"\"\n",
    "        else:\n",
    "            return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform: Final DT API Processing Function.\n",
    "\n",
    "\n",
    "def process_dt_api(dt, resp, df_act_emp):\n",
    "    emp_logged = []\n",
    "    today = resp['employees'][dt]\n",
    "    for emp in today:\n",
    "        emp_logged.append(emp)\n",
    "\n",
    "    remove_cols = ['profileUrl', 'activeProject', 'notes', 'afterWorkTime', 'beforeWorkTime',\n",
    "                   'work_starts', 'work_ends', 'arrived', 'left', 'late', 'isOnline']\n",
    "    cols = []\n",
    "\n",
    "    for x in resp['employees'][dt][emp_logged[0]]:\n",
    "        if not remove_cols.__contains__(x):\n",
    "            cols.append(x)\n",
    "    dict1 = {}\n",
    "    for col in cols:\n",
    "        dict1[col] = []\n",
    "\n",
    "    for emp in emp_logged:\n",
    "        tmp = resp['employees'][dt][emp]\n",
    "        dict1['id'].append(str(tmp['id']))\n",
    "        dict1['name'].append(tmp['name'])\n",
    "        dict1['email'].append(tmp['email'])\n",
    "        dict1['groupId'].append(tmp[\"groupId\"])\n",
    "        dict1['group'].append(tmp[\"group\"])\n",
    "        dict1['onlineTime'].append(tmp[\"onlineTime\"])\n",
    "        dict1['offlineTime'].append(tmp[\"offlineTime\"])\n",
    "        dict1['desktimeTime'].append(tmp[\"desktimeTime\"])\n",
    "        dict1['atWorkTime'].append(tmp[\"atWorkTime\"])\n",
    "        dict1['productiveTime'].append(tmp[\"productiveTime\"])\n",
    "        dict1['productivity'].append(tmp[\"productivity\"])\n",
    "        dict1['efficiency'].append(tmp[\"efficiency\"])\n",
    "\n",
    "    df1 = pd.DataFrame(dict1)\n",
    "    df1.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    df_merged = df_act_emp.merge(df1,\n",
    "                                 left_on=\"Email\",\n",
    "                                 right_on=\"email\",\n",
    "                                 suffixes=(\"_left\", \"_right\"),\n",
    "                                 how=\"left\")\n",
    "    df_merged[\"date\"] = dt\n",
    "    df_merged[\"count\"] = 1\n",
    "    print(\"Complete Data converted to a Dataframe.\")\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "def update_exist_emp(exist_emp_df, df_dt_usage):\n",
    "    df_tmp = df_dt_usage[[\"Emp ID\", \"Reporting Manager\",\n",
    "                          \"Department\", \"Division\", \"id\"]]\n",
    "    df_merged = exist_emp_df.merge(\n",
    "        df_tmp,\n",
    "        left_on=\"Emp ID\",\n",
    "        right_on=\"Emp ID\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_existing\", \"_new\")\n",
    "    )\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform2: Get App Usage from Desktime per employee.\n",
    "\n",
    "\n",
    "df_app_list = pd.DataFrame(columns=[\n",
    "                           'Date', 'Name', 'App_Name', 'App_Type', 'App_Tag', 'Time_Mins', 'Category', 'Productive'])\n",
    "\n",
    "\n",
    "def get_meeting_time(x):\n",
    "    if x.__contains__('teams') | x.__contains__('CALENDAR') | x.__contains__('Teams'):\n",
    "        return \"Teams\"\n",
    "    else:\n",
    "        return \"Work\"\n",
    "\n",
    "\n",
    "def get_app_usage(emp_id, emp_resp, dt):\n",
    "    dict_usage = {\"Date\": [], \"Desktime_ID\": [], \"emp_app_name\": [], \"emp_time_mins\": [\n",
    "    ], \"emp_time_secs\": [], \"emp_category\": [], \"emp_productive\": []}\n",
    "    # print(emp_resp)\n",
    "    app_usage = emp_resp[\"apps\"]['1']\n",
    "\n",
    "    for app in app_usage:\n",
    "        t_spent_mins = np.round(\n",
    "            int(emp_resp[\"apps\"][\"1\"][app][\"duration\"])/60, 2)\n",
    "        t_spent_secs = int(emp_resp[\"apps\"][\"1\"][app][\"duration\"])\n",
    "        app_name = emp_resp[\"apps\"][\"1\"][app][\"app\"]\n",
    "        app_type = emp_resp[\"apps\"][\"1\"][app][\"type\"]\n",
    "        dict_usage[\"Date\"].append(dt)\n",
    "        dict_usage[\"Desktime_ID\"].append(emp_id)\n",
    "        dict_usage[\"emp_app_name\"].append(app_name)\n",
    "        dict_usage[\"emp_time_mins\"].append(t_spent_mins)\n",
    "        dict_usage[\"emp_time_secs\"].append(t_spent_secs)\n",
    "        dict_usage[\"emp_category\"].append(get_meeting_time(app_name))\n",
    "        dict_usage[\"emp_productive\"].append(\"Productive\")\n",
    "\n",
    "    return pd.DataFrame(dict_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load: Main Function\n",
    "\n",
    "def main(n=1, location=\"\", strt_dt=\"\"):\n",
    "    '''\n",
    "    n = Number of days back you want to look for the data from Desktime. Default value is 1. \n",
    "\n",
    "    merge_all = Create a Mega Merged file containing all data at one place. Takes a lot of storage. Default value is False. \n",
    "\n",
    "    location = The place where files need to be saved and from where credentials will be picked. \n",
    "    There should be two folders under the main location: \n",
    "    /Creds, which will contain a file called dt_cred.json. \n",
    "    /Data_Files, where all the data will be dumped. \n",
    "\n",
    "    You may specify a custom location but the API file, Existing Employee File, and folders need to be present at that location. \n",
    "\n",
    "    If Start Date (strt_dt) is provided, the script will walk back n days from start date and give those dates as details.\n",
    "    It should be in YYYY-MM-DD format\n",
    "    '''\n",
    "    date_format = '%Y-%m-%d'\n",
    "\n",
    "    # Define the location of Creds File and Variables\n",
    "    if location == \"\":\n",
    "        loc = \"C:/Users/ShashankRaj/OneDrive - Exalogic Consulting/Documents/Proj Mgmt/Jira_Clockwork\"\n",
    "        # loc = \"C:/Users/shash/OneDrive/Private Documents/My Learning Repo/Python/Proj Mgmt\"\n",
    "    else:\n",
    "        loc = location\n",
    "\n",
    "    cred_loc = loc+\"/Creds\"\n",
    "    Data_loc = loc+\"/Data_Files\"\n",
    "    cloud_loc = \"G:/My Drive/Exalogic Internal Data\"\n",
    "    # cloud_loc=\"C:/Users/shash/OneDrive/Private Documents/My Learning Repo/Python/Proj Mgmt/Cloud_Drive\"\n",
    "\n",
    "    if os.path.exists(cred_loc) & os.path.exists(Data_loc):\n",
    "        cred_file = cred_loc+\"/dt_cred.json\"\n",
    "        df_emp = pd.read_csv(Data_loc+\"/Act_Emp.csv\")\n",
    "    else:\n",
    "        print(\"Unable to proceed without Credentials.\")\n",
    "        os.abort()\n",
    "\n",
    "    DT_App_Usage = pd.DataFrame(columns=[\n",
    "                                \"Date\",\t\"Desktime_ID\",\t\"emp_app_name\", \"emp_time_mins\", \"emp_category\", \"emp_productive\"])\n",
    "\n",
    "    DT_EMP_List = pd.DataFrame(columns=[\"STATUS\", \"Emp ID\", \"Name\", \"Email\", \"Reporting Manager\", \"Department\", \"Division\", \"Internal/Contract\", \"id\", \"group\",\n",
    "                                        \"onlineTime\", \"productiveTime\", \"productivity\", \"efficiency\", \"date\", \"count\", \"Last Name \"])\n",
    "\n",
    "    # Get the date range for which this code will execute\n",
    "    date_range = []\n",
    "    if len(strt_dt) < 2:\n",
    "        curr_dt = str(datetime.today()-timedelta(days=int(0)+1)).split(\" \")[0]\n",
    "        date_range.append(curr_dt)\n",
    "        print(f'Start date is {curr_dt}')\n",
    "    else:\n",
    "        for dt in np.arange(n):\n",
    "            curr_dt = str(datetime.strptime(strt_dt, date_format) -\n",
    "                          timedelta(days=int(dt)+1)).split(\" \")[0]\n",
    "            date_range.append(curr_dt)\n",
    "\n",
    "    # Read the credentials\n",
    "    api_key, emp_url, app_url = read_creds(cred_file, main=1)\n",
    "\n",
    "    # Check if we got the API key. If yes, we proceed with getting API Data from Desktime for evety date object.\n",
    "    if len(api_key) > 2:\n",
    "        for dt_val in date_range:\n",
    "            print(\"*\"*30)\n",
    "            print(f\"\\n Processing Data for {dt_val} \")\n",
    "            # TODO: asyncio can be used here to read multiple dates in a go.\n",
    "            json_data = get_dt_data(url=emp_url, api=api_key, dt=dt_val)\n",
    "\n",
    "            # Gives the Employees' desktime usage\n",
    "            df_stg1 = process_dt_api(\n",
    "                dt=dt_val, resp=json_data, df_act_emp=df_emp)\n",
    "\n",
    "            # Getting the employee list for all employee Data for the given date range\n",
    "            DT_EMP_List = pd.concat([DT_EMP_List, df_stg1])\n",
    "\n",
    "            # Getting all the Employees whose reference is present in Desktime.\n",
    "            t_emp = df_stg1[~df_stg1[\"id\"].isna()][\"id\"]\n",
    "\n",
    "            # Enrich Employee list before Getting App Usage\n",
    "            enrichd_list = update_exist_emp(df_emp, df_stg1)\n",
    "\n",
    "            print(f\"Total {len(t_emp)} employees.\")\n",
    "\n",
    "            # Getting the individual app usage for each employee.\n",
    "            progress_bar(0, len(t_emp), \"\")\n",
    "            for i, emp in enumerate(t_emp):\n",
    "\n",
    "                # TODO: This is a Time expensive call. Should be pushed to multiprocessing module.\n",
    "                app_usg_json = get_dt_data(app_url, api_key, dt_val, emp=emp)\n",
    "                df_stg2 = get_app_usage(emp, app_usg_json, dt_val)\n",
    "\n",
    "                # All Employees' usage data is here.\n",
    "                DT_App_Usage = pd.concat([DT_App_Usage, df_stg2])\n",
    "\n",
    "                progress_bar(i+1, len(t_emp), emp)\n",
    "\n",
    "        # TODO: This is a space extensive call. Change the file format for final processing.\n",
    "        # print(DT_App_Usage.columns)\n",
    "        # print(enrichd_list.columns)\n",
    "\n",
    "        DT_App_Usage = DT_App_Usage.merge(\n",
    "            enrichd_list,\n",
    "            left_on=\"Desktime_ID\",\n",
    "            right_on=\"id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # print(DT_EMP_List.columns)\n",
    "        # print(DT_App_Usage.columns)\n",
    "\n",
    "        DT_EMP_List = DT_EMP_List[[\"STATUS\", \"Emp ID\", \"Name\", \"Email\", \"Reporting Manager\", \"Department\", \"Division\", \"Internal/Contract\", \"id\", \"group\",\n",
    "                                   \"onlineTime\", \"productiveTime\", \"productivity\", \"efficiency\", \"date\", \"count\", \"Last Name \"]]\n",
    "\n",
    "        DT_App_Usage = DT_App_Usage[[\"Date\", \"Desktime_ID\", \"emp_app_name\", \"emp_category\", \"emp_time_secs\", \"Emp ID\", \"Name\", \"Last Name \",\n",
    "                                     \"Email\", \"Reporting Manager_existing\", \"Department_existing\", \"Division_existing\",\n",
    "                                     \"Internal/Contract\"]]\n",
    "\n",
    "        # ====================== Fix Some Data Issues with DT_EMP_List ==================================\n",
    "        # Concatenating the Names\n",
    "        DT_EMP_List[\"Full_Name\"] = DT_EMP_List[\"Name\"] + \\\n",
    "            \" \"+DT_EMP_List[\"Last Name \"]\n",
    "\n",
    "        # Fixing the Employee IDs\n",
    "        DT_EMP_List[\"Emp ID\"].fillna(0000, inplace=True)\n",
    "        DT_EMP_List[\"id\"].fillna(0000, inplace=True)\n",
    "\n",
    "        DT_EMP_List[\"Emp ID\"] = DT_EMP_List[\"Emp ID\"].astype(\"str\")\n",
    "        DT_EMP_List[\"id\"] = DT_EMP_List[\"id\"].astype(\"str\")\n",
    "\n",
    "        DT_EMP_List[\"Emp ID\"] = DT_EMP_List[\"Emp ID\"].str.split(\".\", expand=True)[\n",
    "            0]\n",
    "        DT_EMP_List[\"id\"] = DT_EMP_List[\"id\"].str.split(\".\", expand=True)[0]\n",
    "\n",
    "        # Fixing Reporting Manager\n",
    "        DT_EMP_List[\"Reporting Manager\"].fillna(\"HOD\", inplace=True)\n",
    "\n",
    "        # If onlinetime blank -> (-1)\n",
    "        DT_EMP_List[\"onlineTime\"].fillna(\"-1\", inplace=True)\n",
    "        DT_EMP_List.drop(columns=['Name', 'Last Name '], inplace=True)\n",
    "        DT_EMP_List = DT_EMP_List.iloc[:, [\n",
    "            13, 15, 1, 7, 2, 0, 3, 5, 4, 6, 8, 9, 10, 11, 12, 14]]\n",
    "\n",
    "        # ================================================================================================\n",
    "\n",
    "        DT_App_Usage.columns = [\"Date\", \"Desktime_ID\", \"emp_app_name\", \"emp_category\", \"emp_time_secs\", \"Emp ID\", \"Name\", \"Last Name \",\n",
    "                                \"Email\", \"Reporting Manager\", \"Department\", \"Division\", \"Internal/Contract\"]\n",
    "\n",
    "        if os.path.exists(Data_loc+\"/Desktime_usage.csv\"):\n",
    "            print(\"File Already exists. Will Skip the header.\")\n",
    "            try:\n",
    "                DT_EMP_List.to_csv(cloud_loc+\"/Desktime_usage.csv\",\n",
    "                                   index=False, mode='a', header=False)\n",
    "                DT_App_Usage.to_csv(\n",
    "                    cloud_loc+\"/DT_App_Usage.csv\", index=False, mode='a', header=False)\n",
    "\n",
    "                DT_EMP_List.to_csv(Data_loc+\"/Desktime_usage.csv\",\n",
    "                                   index=False, mode='a', header=False)\n",
    "                DT_App_Usage.to_csv(\n",
    "                    Data_loc+\"/DT_App_Usage.csv\", index=False, mode='a', header=False)\n",
    "            except FileNotFoundError:\n",
    "                print(\"Unable to export to Excel Sheet.\")\n",
    "            else:\n",
    "                print(\"Exported the data to Excel Sheets.\")\n",
    "        else:\n",
    "            try:\n",
    "                DT_EMP_List.to_csv(\n",
    "                    cloud_loc+\"/Desktime_usage.csv\", index=False, mode='a')\n",
    "                DT_App_Usage.to_csv(\n",
    "                    cloud_loc+\"/DT_App_Usage.csv\", index=False, mode='a')\n",
    "\n",
    "                DT_App_Usage.to_csv(\n",
    "                    Data_loc+\"/DT_App_Usage.csv\", index=False, mode='a')\n",
    "                DT_EMP_List.to_csv(\n",
    "                    Data_loc+\"/Desktime_usage.csv\", index=False, mode='a')\n",
    "            except FileNotFoundError:\n",
    "                print(\"Unable to export to Excel Sheet.\")\n",
    "            else:\n",
    "                print(\"Exported the data to Excel Sheet.\")\n",
    "    return \"Process Successful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the Jira Data.\n",
    "\n",
    "def load_json_file(file_name):\n",
    "    if not file_name.endswith(\".json\"):\n",
    "        print(\"Only Josn file is expected. Quitting...\")\n",
    "        return\n",
    "    else:\n",
    "        try:\n",
    "            f = open(file_name, encoding='utf8')\n",
    "        except Exception:\n",
    "            return Exception\n",
    "        else:\n",
    "            data = json.load(f)\n",
    "            print(\"Loaded Json Data\")\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_json_data(raw_json, df_act_emp, dt):\n",
    "    dict = {\n",
    "        \"user_names\": [],\n",
    "        \"email_add\": [],\n",
    "        \"jira_ids\": [],\n",
    "        \"issue_key\": [],\n",
    "        \"issue_id\": [],\n",
    "        \"created\": [],\n",
    "        \"started\": [],\n",
    "        \"updated\": [],\n",
    "        \"time_secs\": []\n",
    "    }\n",
    "\n",
    "    for x in raw_json:\n",
    "        dict['user_names'].append(x[\"author\"][\"displayName\"])\n",
    "        dict['email_add'].append(x[\"author\"][\"emailAddress\"])\n",
    "        dict['jira_ids'].append(x[\"id\"])\n",
    "        dict['issue_key'].append(x[\"issue\"][\"key\"])\n",
    "        dict['issue_id'].append(x[\"issueId\"])\n",
    "        dict['time_secs'].append(x[\"timeSpentSeconds\"])\n",
    "        dict[\"created\"].append(x[\"created\"])\n",
    "        dict[\"updated\"].append(x[\"updated\"])\n",
    "        dict[\"started\"].append(x[\"started\"])\n",
    "\n",
    "    df_jira = pd.DataFrame(dict)\n",
    "    df_jira[\"created\"] = pd.to_datetime(df_jira[\"created\"])\n",
    "    df_jira[\"started\"] = pd.to_datetime(df_jira[\"started\"])\n",
    "    df_jira[\"updated\"] = pd.to_datetime(df_jira[\"updated\"])\n",
    "\n",
    "    df_jira[\"created\"] = df_jira[\"created\"].dt.date\n",
    "    df_jira[\"started\"] = df_jira[\"started\"].dt.date\n",
    "    df_jira[\"updated\"] = df_jira[\"updated\"].dt.date\n",
    "\n",
    "    df_merged_act_emp = df_act_emp.merge(\n",
    "        df_jira,\n",
    "        how='left',\n",
    "        left_on=\"Email\",\n",
    "        right_on=\"email_add\"\n",
    "    )\n",
    "\n",
    "    # df_merged_act_emp[\"date_issue\"] = str(datetime.today()-timedelta(days=int(0)+1)).split(\" \")[0]\n",
    "    # curr_dt = str(datetime.today()-timedelta(days=int(0)+1)).split(\" \")[0]\n",
    "    df_merged_act_emp[\"date_issue\"] = str(\n",
    "        datetime.strptime(dt, \"%Y-%m-%d\").date())\n",
    "\n",
    "    df_merged_act_emp[\"count\"] = 1\n",
    "\n",
    "    # ==================================== Fixing Data Issues in the Dataframe ============================\n",
    "\n",
    "    df_merged_act_emp[\"Full_Name\"] = df_merged_act_emp[\"Name\"] + \\\n",
    "        \" \"+df_merged_act_emp[\"Last Name \"]\n",
    "    df_merged_act_emp.drop(\n",
    "        columns=['user_names', 'email_add', 'jira_ids', 'Name', 'Last Name '], inplace=True)\n",
    "    df_merged_act_emp[\"issue_key\"].fillna(\"NA\", inplace=True)\n",
    "    df_merged_act_emp[\"Project\"] = df_merged_act_emp[\"issue_key\"].str.split(\n",
    "        \"-\", expand=True)[0]\n",
    "    df_merged_act_emp['Emp ID'] = df_merged_act_emp['Emp ID'].astype(\n",
    "        'str').str.split(\".\", expand=True)[0]\n",
    "    df_merged_act_emp = df_merged_act_emp.iloc[:, [\n",
    "        13, 16, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14]]\n",
    "\n",
    "    # =====================================================================================================\n",
    "\n",
    "    return df_merged_act_emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jira(proc_dt, loc=\"\"):\n",
    "    '''\n",
    "    dt = Date for which this file will be processed. \n",
    "    loc = Location of the jira Folder where the log.json file is stored. \n",
    "\n",
    "    '''\n",
    "    if os.path.exists(loc):\n",
    "        location = loc\n",
    "    else:\n",
    "        location = \"C:/Users/ShashankRaj/OneDrive - Exalogic Consulting/Documents/Proj Mgmt/Jira_Clockwork\"\n",
    "        # location = \"C:/Users/shash/OneDrive/Private Documents/My Learning Repo/Python/Proj Mgmt\"\n",
    "    cloud_loc = \"G:/My Drive/Exalogic Internal Data\"\n",
    "    df_act_emp = pd.read_csv(location+\"/Data_Files/Act_Emp.csv\")\n",
    "    # df_act_emp = act_emp\n",
    "\n",
    "    jira_data = load_json_file(location+\"/Data_Files/log.json\")\n",
    "    final_df = transform_json_data(jira_data, df_act_emp, dt=proc_dt)\n",
    "\n",
    "    if os.path.exists(location+\"/Data_Files/Jira_Data.csv\"):\n",
    "        final_df.to_csv(location+\"/Data_Files/Jira_Data.csv\",\n",
    "                        mode='a', header=False, index=False)\n",
    "        final_df.to_csv(cloud_loc+\"/Jira_Data.csv\",\n",
    "                        mode='a', header=False, index=False)\n",
    "    else:\n",
    "        final_df.to_csv(location+\"/Data_Files/Jira_Data.csv\",\n",
    "                        mode='a', index=False)\n",
    "        final_df.to_csv(cloud_loc+\"/Jira_Data.csv\",\n",
    "                        mode='a', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    dt_extract = input(\"Process Desktime Data? (Y/N):\")\n",
    "    if dt_extract.lower() == \"y\":\n",
    "        print(f'\\n')\n",
    "        print(\"*\"*25)\n",
    "        print(\"This Script takes the following arguments:\")\n",
    "        print(\n",
    "            \"n = [int]. By Default it is 1 and will always look at today -1 date.\")\n",
    "        print(\"merge_all = True/False. Whether to merge all the data and give one excel with all information. This is very storage heavy. \")\n",
    "        print(\"strt_dt = YYYY-MM-DD. If we need to look back n days, then define start date and n.\")\n",
    "        print(\"For Example, if we need to look back 5 days from 24-Jan-2023, then n = 5 and start_dt = '2023-01-24'\")\n",
    "        print(\"Default is Today\")\n",
    "        past_dt = input(\"Enter Start Date: \")\n",
    "        past_n = input(\"Enter No of Days to look back: \")\n",
    "\n",
    "        if len(past_n):\n",
    "            past_n = int(past_n)\n",
    "        else:\n",
    "            past_n = 1\n",
    "\n",
    "        print(main(strt_dt=past_dt,\n",
    "                   n=past_n))\n",
    "\n",
    "    jira_extract = input(\"Extract Jira Data? (Y/N): \")\n",
    "    if jira_extract.lower() == \"y\":\n",
    "        dt_obj = input(\n",
    "            \"Date for which this file gets processed (YYYY-MM-DD) : \")\n",
    "        process_jira(dt_obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
