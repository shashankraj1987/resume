{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import log_config as lc\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'posix':\n",
    "    file_loc = r\"/home/shashankraj/Documents/DATA_Dump\"\n",
    "    log_file = file_loc+\"/\"+\"Logs\"\n",
    "    final_files = file_loc+\"/\"+\"Final_Df\"\n",
    "    log_file = file_loc+\"/\"+\"Logs\"\n",
    "    backup = file_loc+\"/\"+\"Backup\"\n",
    "    #today = str(datetime.today().date())\n",
    "    trigger_file = file_loc+\"/file_trigger/\"+\"new_data_received.txt\"\n",
    "else:\n",
    "    file_loc=r\"D:\\One Drive Anza\\OneDrive - Anza Services LLP\\DATA_Dump\\Final_Df\"\n",
    "    log_file = file_loc+\"\\\\\"+\"Logs\"\n",
    "    log_file = file_loc+\"\\\\\"+\"Logs\"\n",
    "    final_files = file_loc+\"\\\\\"+\"Final_Df\"\n",
    "    backup = file_loc+\"\\\\\"+\"Backup\"\n",
    "    #today = str(datetime.today().date())\n",
    "    trigger_file = file_loc+\"\\\\file_trigger\\\\\"+\"new_data_received.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_rows = {'Client Billing Descending': 0, \"Fee Breakdown by Dept and Fee Earner\": 3,\n",
    "             \"Fee Summary by Dept and Fee Earner\": 3, \"Fees Billed\": 3, \"Matter Source of Business inc Matter Bills\": 0,\n",
    "             \"Matters Opened by FE\": 3, \"Payment Received Analysis\": 3, \"Total Hours by Fee Earner-With Billings\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols(df):\n",
    "    \"\"\" \n",
    "        This will remove all the columns that contain the word Textbox in them. \n",
    "        This Function takes a DataFrame as in input and returns all the columns except TextBox. \n",
    "    \"\"\"\n",
    "\n",
    "    cols = df.columns\n",
    "    new_cols = []\n",
    "    txt_chk = re.compile(r'Textbox')\n",
    "    tot_hrs_col_name = [\"RecordedHours2\",\"NonChargeHours2\",\"WOHours2\",\"TotalHour2\",\"bankRef\"]\n",
    "    new_cols = [col_name for col_name in cols if not(txt_chk.search(col_name)) and col_name not in tot_hrs_col_name]\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(dct, match):\n",
    "    \"\"\"\n",
    "        Takes a Dictionary and a filename as inputs and Returns how many rows need to be skipped for a filename. \n",
    "        Returns the Number of rows to skip, while creating a DataFrame.\n",
    "    \"\"\"\n",
    "    for val in dct.keys():\n",
    "        if re.match(val, match):\n",
    "            return dct[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_Filename(fname):\n",
    "    \"\"\"\n",
    "        Accepts a Filename that has fname_date.csv format. \n",
    "        It Extracts the From Date form the File and Returns the same. \n",
    "        These Are Datetime Objects.  \n",
    "        If the Filename has only start date, it will just return the same date for Both Start and End Date. \n",
    "    \"\"\"\n",
    "\n",
    "    pattern = re.compile(r'_\\d*')\n",
    "    match = pattern.findall(fname)\n",
    "    dt = match[0]\n",
    "    dt = dt.split(\"_\")[1]\n",
    "    file_date = pd.to_datetime(dt, format='%d%m%Y')\n",
    "    return file_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_files(file_loc):\n",
    "    import sys\n",
    "    \n",
    "    if os.name == 'posix':\n",
    "            os.system('clear')\n",
    "    else:\n",
    "        os.system('cls')\n",
    "\n",
    "    log_loc = file_loc + \"/\" + \"Logs\"\n",
    "    unprocessed = file_loc + \"/Unprocessed\"\n",
    "    processed = file_loc + \"/Processed\"\n",
    "    ess_fold_list = [log_loc,unprocessed,processed]\n",
    "\n",
    "    cat_file_logger = lc.start_log(log_loc)\n",
    "    \n",
    "    for fold in ess_fold_list:\n",
    "        if os.path.exists(fold):\n",
    "            cat_file_logger.info(f'\\n[{fold}] already Exists in [{file_loc}]\\n')\n",
    "        else:\n",
    "            os.mkdir(file_loc+\"/\"+fold)\n",
    "            cat_file_logger.info(f'\\nCreating Folder {fold} in {file_loc}\\n')\n",
    "\n",
    "    total_csv = len([f for f in os.listdir(file_loc) if f.endswith('.csv')])\n",
    "    if total_csv > 0:\n",
    "        print(f\"Found {total_csv} csv files\")\n",
    "    elif total_csv == 0:\n",
    "        print(\"No CSV Files Found. Exiting.\")\n",
    "        sys.exit()\n",
    "\n",
    "    process_files = {}\n",
    "    discard_files = {}\n",
    "    file_list = os.listdir(file_loc)\n",
    "\n",
    "    cat_file_logger.info(f\"Any xls file and files having Pie in the names will not be Processed\")\n",
    "\n",
    "    # Segregating the Files.\n",
    "    discard_files['all_pie'] = [files for files in file_list if len(re.compile(r'[\\sa-zA-Z\\s]+Pie \\w+_\\d+.csv').findall(files))]\n",
    "    discard_files['all_xlsx'] = [files for files in file_list if files.endswith(\".xlsx\")]\n",
    "\n",
    "    # Move the above files to Unprocessed Folder before moving ahead\n",
    "    for f in discard_files.keys():\n",
    "                cat_file_logger.info(f'Moving out files of list [{f}] to folder [{unprocessed}]')\n",
    "                try:\n",
    "                    [shutil.move(file_loc + \"/\" + file, unprocessed)  for file in discard_files[f]]\n",
    "                except:\n",
    "                    cat_file_logger.error(\"File Already exists in Destination\")\n",
    "                else:\n",
    "                    cat_file_logger.info(\"Files moved\")\n",
    "\n",
    "\n",
    "    process_files['client_billing'] = [files for files in file_list if len(re.compile(r'Client [a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['client_billing'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Client BIlling ')\n",
    "    \n",
    "    process_files['fee_brkdn_dept_fe'] = [files for files in file_list if len(re.compile(r'Fee Breakdown [a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['fee_brkdn_dept_fe'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Fee Breakdown by Dept ')\n",
    "    \n",
    "    process_files['fee_summ_dept_fe'] = [files for files in file_list if len(re.compile(r'Fee Summary [a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['fee_summ_dept_fe'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Fee Summary by Dept ')\n",
    "        \n",
    "    process_files['fees_billed'] = [files for files in file_list if len(re.compile(r'Fees B[a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['fees_billed'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Fees BIlled ')\n",
    "        \n",
    "    process_files['matter_src'] = [files for files in file_list if len(re.compile(r'Matter Source [a-zA-Z\\s()]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['matter_src'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Matter Source Reference ')\n",
    "     \n",
    "    process_files['matter_opened'] = [files for files in file_list if len(re.compile(r'Matters Open[\\sa-zA-Z\\s()]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['matter_opened'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Matter Opened by FE ')\n",
    "    \n",
    "    process_files['payment_rcv'] = [files for files in file_list if len(re.compile(r'Payment [\\sa-zA-Z\\s()]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['payment_rcv'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Payment Received ')\n",
    "       \n",
    "    process_files['tot_hrs_fe'] = [files for files in file_list if len(re.compile(r'([tT]otal[\\sa-z-A-Z\\s]*_\\d+.csv)').findall(files))]\n",
    "    cnt = len(process_files['tot_hrs_fe'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Total Hours by Fee Earner ')\n",
    "\n",
    "    for f in process_files.keys():\n",
    "        cat_file_logger.info(f'Moving category [{f}]')\n",
    "        final_file = file_loc + \"/\" + file\n",
    "        try:\n",
    "            [shutil.move(final_file, processed)  for file in process_files[f]]\n",
    "            #[cat_file_logger.info(f' Moving files {file} to processed')  for file in process_files[f]]\n",
    "        except:\n",
    "            [shutil.move(final_file, processed) if os.path.exists(file_loc + \"/\" + file) else os.remove(processed+\"/\"+final_file) & shutil.move(final_file, processed) for file in process_files[f]]\n",
    "            cat_file_logger.info(\"File already Existed, Moving Latest version of [{}]\".format(f))\n",
    "        else:\n",
    "            [cat_file_logger.info(f'Moving File --> {file}')  for file in process_files[f]]\n",
    "            pass\n",
    "        \n",
    "    return process_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prime', 2, 'Prime', 4, 'Prime', 6, 'Prime', 8, 'Prime']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list = (1,2,3,4,5,6,7,8,9)\n",
    "\n",
    "[x if x%2 == 0 else \"Prime\" for x in p_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_files2(file_loc):\n",
    "\n",
    "    import sys\n",
    "    \n",
    "    if os.name == 'posix':\n",
    "            os.system('clear')\n",
    "    else:\n",
    "        os.system('cls')\n",
    "\n",
    "    log_loc = file_loc + \"/Logs\"\n",
    "    cat_file_logger = lc.start_log(log_loc)\n",
    "    unprocessed = file_loc + \"/Unprocessed\"\n",
    "    processed = file_loc + \"/Processed\"\n",
    "\n",
    "    ess_fold_list = [log_loc,unprocessed,processed]\n",
    "\n",
    "    for fold in ess_fold_list:\n",
    "        if os.path.exists(fold):\n",
    "            cat_file_logger.info(f'\\n[{fold}] already Exists in [{file_loc}]\\n')\n",
    "        else:\n",
    "            os.mkdir(file_loc+\"/\"+fold)\n",
    "            cat_file_logger.info(f'\\nCreating Folder {fold} in {file_loc}\\n')\n",
    "\n",
    "    total_csv = len([f for f in os.listdir(file_loc) if f.endswith('.csv')])\n",
    "    if total_csv > 0:\n",
    "        cat_file_logger.info(f\"Found {total_csv} csv files\")\n",
    "    elif total_csv == 0:\n",
    "        cat_file_logger.error(\"No CSV Files Found. Exiting.\")\n",
    "        sys.exit()\n",
    "\n",
    "    process_files = {}\n",
    "    discard_files = {}\n",
    "    file_list = os.listdir(file_loc)\n",
    "\n",
    "    cat_file_logger.info(f\"Any xls file and files having Pie in the names will not be Processed\")\n",
    "\n",
    "    # Segregating the Files.\n",
    "    discard_files['all_pie'] = [files for files in file_list if len(re.compile(r'[\\sa-zA-Z\\s]+Pie \\w+_\\d+.csv').findall(files))]\n",
    "    discard_files['all_xlsx'] = [files for files in file_list if files.endswith(\".xlsx\")]\n",
    "\n",
    "    # Move the above files to Unprocessed Folder before moving ahead\n",
    "    for f in discard_files.keys():\n",
    "                cat_file_logger.info(f'Moving out files of list [{f}] to folder [{unprocessed}]')\n",
    "                # [shutil.move(file_loc + \"/\" + file, unprocessed)  for file in discard_files[f]]\n",
    "                [cat_file_logger.info(f\"Moving File {file} to  unprocessed\")  for file in discard_files[f]]\n",
    "\n",
    "    process_files['client_billing'] = [files for files in file_list if len(re.compile(r'Client [a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['client_billing'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Client BIlling ')\n",
    "    \n",
    "    process_files['fee_brkdn_dept_fe'] = [files for files in file_list if len(re.compile(r'Fee Breakdown [a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['fee_brkdn_dept_fe'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Fee Breakdown by Dept ')\n",
    "    \n",
    "    process_files['fee_summ_dept_fe'] = [files for files in file_list if len(re.compile(r'Fee Summary [a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['fee_summ_dept_fe'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Fee Summary by Dept ')\n",
    "        \n",
    "    process_files['fees_billed'] = [files for files in file_list if len(re.compile(r'Fees B[a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['fees_billed'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Fees BIlled ')\n",
    "        \n",
    "    process_files['matter_src'] = [files for files in file_list if len(re.compile(r'Matter Source [a-zA-Z\\s()]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['matter_src'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Matter Source Reference ')\n",
    "     \n",
    "    process_files['matter_opened'] = [files for files in file_list if len(re.compile(r'Matters Open[\\sa-zA-Z\\s()]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['matter_opened'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Matter Opened by FE ')\n",
    "    \n",
    "    process_files['payment_rcv'] = [files for files in file_list if len(re.compile(r'Payment [\\sa-zA-Z\\s()]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['payment_rcv'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Payment Received ')\n",
    "       \n",
    "    process_files['tot_hrs_fe'] = [files for files in file_list if len(re.compile(r'([tT]otal[\\sa-z-A-Z\\s]*_\\d+.csv)').findall(files))]\n",
    "    cnt = len(process_files['tot_hrs_fe'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Total Hours by Fee Earner ')\n",
    "\n",
    "    for f in process_files.keys():\n",
    "        cat_file_logger.info(f'Moving category [{f}]')\n",
    "        try:\n",
    "            #[shutil.move(file_loc + \"/\" + file, processed)  for file in process_files[f]]\n",
    "            [cat_file_logger.info(f' Moving files {file} to processed')  for file in process_files[f]]\n",
    "        except:\n",
    "            cat_file_logger.info(\"Error Moving File {}\".format(f))\n",
    "        else:\n",
    "            #[cat_file_logger.info(f'Moving File --> {file}')  for file in process_files[f]]\n",
    "            pass\n",
    "        \n",
    "    return process_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_files(dict_list, file_loc, logfile_loc):\n",
    "    concat_logger = lc.start_log(logfile_loc)\n",
    "    df_all_files = {}\n",
    "    dict_fname= \"\"\n",
    "     # If a Filename is not in this Dictionary, then it will not be Considered. \n",
    "    # date = (datetime.now()).strftime(\"%m-%d-%y\")\n",
    "    concat_logger.info(f'Following Keys will be processed - [{dict_list.keys()}]')\n",
    "    for file_cat in dict_list.keys():\n",
    "        df_final = pd.DataFrame()\n",
    "        concat_logger.info('*' * 50)\n",
    "        concat_logger.info(f'Processing Category {file_cat}')\n",
    "        for file in dict_list[file_cat]:\n",
    "            dict_fname = file.split(\"_\")[0]\n",
    "            dfc_file = pd.read_csv((file_loc + \"/Processed/\" + file), skiprows=get_rows(skip_rows, file))\n",
    "            dfc_file = dfc_file[remove_cols(dfc_file)]\n",
    "            processing_date = get_date_from_Filename(file)\n",
    "            dfc_file[\"Date_Added\"] = processing_date\n",
    "            df_final = pd.concat([df_final, dfc_file], ignore_index=True)\n",
    "            df_final.fillna(0)\n",
    "            df_final = df_final.replace(re.compile(r'Â£'), \"\").replace(re.compile(r','), \"\").replace(re.compile(r'\\('),\"-\").replace(re.compile(r'\\)'), \"\")\n",
    "\n",
    "            for cols in df_final.columns:\n",
    "                try:\n",
    "                    df_final[cols].astype(float)\n",
    "                except:\n",
    "                    continue\n",
    "                    # concat_logger.info(f'Skipping Column {cols}')\n",
    "                else:\n",
    "                    # concat_logger.info(f'Converting {cols} to float')\n",
    "                    df_final[cols] = df_final[cols].astype(float)\n",
    "\n",
    "        df_all_files[dict_fname] = df_final.sort_values(by=\"Date_Added\",ascending=True)\n",
    "\n",
    "    for f in df_all_files.keys():\n",
    "        rows = df_all_files[f].shape[0]\n",
    "        concat_logger.info(f'Will Add -> {rows} entries for [{f}] to the database')\n",
    "\n",
    "    return df_all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Date_Added'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/shashankraj/github/anzabi/Final/notebooks/organize_files.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/shashankraj/github/anzabi/Final/notebooks/organize_files.ipynb#ch0000010?line=0'>1</a>\u001b[0m dict_list \u001b[39m=\u001b[39m categorize_files2(file_loc)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/shashankraj/github/anzabi/Final/notebooks/organize_files.ipynb#ch0000010?line=1'>2</a>\u001b[0m all_files \u001b[39m=\u001b[39m concat_files(dict_list, file_loc, log_file)\n",
      "\u001b[1;32m/home/shashankraj/github/anzabi/Final/notebooks/organize_files.ipynb Cell 11'\u001b[0m in \u001b[0;36mconcat_files\u001b[0;34m(dict_list, file_loc, logfile_loc)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shashankraj/github/anzabi/Final/notebooks/organize_files.ipynb#ch0000009?line=26'>27</a>\u001b[0m                 \u001b[39m# concat_logger.info(f'Skipping Column {cols}')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shashankraj/github/anzabi/Final/notebooks/organize_files.ipynb#ch0000009?line=27'>28</a>\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shashankraj/github/anzabi/Final/notebooks/organize_files.ipynb#ch0000009?line=28'>29</a>\u001b[0m                 \u001b[39m# concat_logger.info(f'Converting {cols} to float')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shashankraj/github/anzabi/Final/notebooks/organize_files.ipynb#ch0000009?line=29'>30</a>\u001b[0m                 df_final[cols] \u001b[39m=\u001b[39m df_final[cols]\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/shashankraj/github/anzabi/Final/notebooks/organize_files.ipynb#ch0000009?line=31'>32</a>\u001b[0m     df_all_files[dict_fname] \u001b[39m=\u001b[39m df_final\u001b[39m.\u001b[39;49msort_values(by\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDate_Added\u001b[39;49m\u001b[39m\"\u001b[39;49m,ascending\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shashankraj/github/anzabi/Final/notebooks/organize_files.ipynb#ch0000009?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m df_all_files\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shashankraj/github/anzabi/Final/notebooks/organize_files.ipynb#ch0000009?line=34'>35</a>\u001b[0m     rows \u001b[39m=\u001b[39m df_all_files[f]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/venv/lib64/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/util/_decorators.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/util/_decorators.py?line=305'>306</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/util/_decorators.py?line=306'>307</a>\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/util/_decorators.py?line=307'>308</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/util/_decorators.py?line=308'>309</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/util/_decorators.py?line=309'>310</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/util/_decorators.py?line=310'>311</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/lib64/python3.10/site-packages/pandas/core/frame.py:6307\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/core/frame.py?line=6302'>6303</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(by):\n\u001b[1;32m   <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/core/frame.py?line=6303'>6304</a>\u001b[0m     \u001b[39m# len(by) == 1\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/core/frame.py?line=6305'>6306</a>\u001b[0m     by \u001b[39m=\u001b[39m by[\u001b[39m0\u001b[39m]\n\u001b[0;32m-> <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/core/frame.py?line=6306'>6307</a>\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_label_or_level_values(by, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/core/frame.py?line=6308'>6309</a>\u001b[0m     \u001b[39m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/core/frame.py?line=6309'>6310</a>\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/core/frame.py?line=6310'>6311</a>\u001b[0m         \u001b[39m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/core/frame.py?line=6311'>6312</a>\u001b[0m         \u001b[39m# \"Series\", variable has type \"ndarray\")\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib64/python3.10/site-packages/pandas/core/generic.py:1848\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/core/generic.py?line=1845'>1846</a>\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mget_level_values(key)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/core/generic.py?line=1846'>1847</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/core/generic.py?line=1847'>1848</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[1;32m   <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/core/generic.py?line=1849'>1850</a>\u001b[0m \u001b[39m# Check for duplicates\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/shashankraj/venv/lib64/python3.10/site-packages/pandas/core/generic.py?line=1850'>1851</a>\u001b[0m \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Date_Added'"
     ]
    }
   ],
   "source": [
    "dict_list = categorize_files2(file_loc)\n",
    "all_files = concat_files(dict_list, file_loc, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in all_files.keys():\n",
    "        all_files[file].columns = [cols.lower() for cols in all_files[file].columns]\n",
    "        fname = final_files+\"/\"+file+\".csv\"\n",
    "        if os.path.exists(fname):\n",
    "            print(f'{fname} already exists')\n",
    "            all_files[file].to_csv(final_files+\"/\"+file+\".csv\",header=False,mode=\"a\",index=False)\n",
    "        else:\n",
    "            print(f'Creating DF {fname}')\n",
    "            all_files[file].to_csv(final_files+\"/\"+file+\".csv\",mode=\"a\",index=False)\n",
    "        all_files[file].to_parquet(backup+\"/\"+file+\".parquet.gzip\",compression = 'gzip')  \n",
    "        print(f'Processed File [{file}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files['Client Billing Descending'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = {}\n",
    "to_add = {}\n",
    "to_update = {}\n",
    "\n",
    "for file in all_files.keys():\n",
    "    print(file)\n",
    "    #tmp_df[file] = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv_file in os.listdir(final_files):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        fname = csv_file.split(\".\")[0]\n",
    "        tmp_df[fname] = pd.read_csv(final_files+\"/\"+csv_file)\n",
    "        tmp_df[fname]['date_added'] = pd.to_datetime(tmp_df[fname]['date_added'])\n",
    "\n",
    "tmp_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df['Client Billing Descending']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files['Client Billing Descending']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for file in all_files.keys():\n",
    "    print(f'Processing {file}')\n",
    "    for index,vals in all_files[file].iterrows():\n",
    "        dt = pd.Timestamp(datetime.date(vals[\"Date_Added\"]))\n",
    "        if dt in tmp_df[file]['date_added'].values:\n",
    "            print(f'Found {dt} in {vals} for {file}')\n",
    "            pd.concat(vals,df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_val = {\"client_surname1\":[\"Newstone 40 Ducks Hill LLP\",\"Cobley Ross\"],\n",
    "# \"total_profit1\":[2600,2590],\n",
    "# \"total_bill_amount\":[3162,3192],\n",
    "# \"total_profit\":[9000,8975],\n",
    "# \"total_bill_amount1\":[10980,10990],\n",
    "# \"date_added\":['2022-03-01','2022-03-01']}\n",
    "\n",
    "# new_val\n",
    "# df1 = pd.DataFrame(new_val)\n",
    "# df1['date_added'] = pd.to_datetime(df1['date_added'])\n",
    "# df1\n",
    "\n",
    "# for index,vals in df1.iterrows():\n",
    "#     clnt = vals[\"client_surname1\"]\n",
    "#     dt = pd.Timestamp(datetime.date(vals[\"date_added\"]))\n",
    "#     print(clnt,\"--\", dt)\n",
    "#     if clnt and dt in df_cb.values:\n",
    "#         print(\"Found\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
