{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import shutil\n",
    "\n",
    "import check_server\n",
    "import log_config\n",
    "import organize_files as of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tmp_fold = os.environ['tmp']\n",
    "except:\n",
    "    print(\"Cannot find temp location. Activities won't be logged\")\n",
    "else:\n",
    "    log_vals = log_config.start_log(tmp_fold)\n",
    "    init_logger = log_vals[0]\n",
    "    print(f'Activities would be initially Logged in {tmp_fold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_REF = {\n",
    "    \"Client Billing Descending\":\"client_billing\",\"Fee Breakdown by Dept and Fee Earner\":\"fee_brkdn_dept_fe\",\"Fee Summary by Dept and Fee Earner\":\"fee_smry_dept_fe\",\n",
    "    \"Fees Billed\":\"fees_billed\",\"Matter Source of Business inc Matter Bills (Bill Date)\":\"mttr_src_ref\",\"Total Hours by Fee Earner-With Billings All\":\"tot_hrs_by_fe\",\n",
    "    \"Matters Opened by FE\":\"mtrs_by_fe\",\"Payment Received Analysis\":\"pmt_rcv_analysis\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_checks(info_file=None):\n",
    "    all_details = {}\n",
    "    info_dict = {}\n",
    "\n",
    "    if info_file is not None:\n",
    "        if os.path.exists(info_file):\n",
    "            init_logger.info(\"Found [{info_file}]\")\n",
    "            all_details[\"loc\"] = info_file\n",
    "\n",
    "            opened_file = open(all_details[\"loc\"], encoding='utf8')\n",
    "            from csv import reader\n",
    "            read_file = reader(opened_file)\n",
    "            db_creds = list(read_file)\n",
    "            opened_file.close()\n",
    "\n",
    "            for info in db_creds:\n",
    "                info_dict[info[0]] = info[1]\n",
    "            \n",
    "            all_details['file_loc'] = info_dict[\"base_loc\"]\n",
    "            all_details['log_file'] = all_details['file_loc']+\"\\\\Logs\"\n",
    "            all_details['final_files'] = all_details['file_loc']+\"\\\\Final_Df\"\n",
    "            all_details['backup'] = all_details['file_loc']+\"\\\\Backup\"\n",
    "            all_details['trigger_file'] = all_details['file_loc']+\"\\\\file_trigger\\\\new_data_received.txt\"\n",
    "\n",
    "            imp_folds = ['file_loc','log_file','final_files','backup','trigger_file']\n",
    "            for fold in imp_folds:\n",
    "                if os.path.exists(all_details[fold]):\n",
    "                    init_logger.info(f\"Found {all_details[fold]}\")\n",
    "                else:\n",
    "                    init_logger.info(f\"Folder {all_details[fold]} Doesn't exist\")\n",
    "\n",
    "            all_details['db'] = info_dict[\"db_name\"]\n",
    "            all_details['db_user'] = info_dict[\"db_user\"]\n",
    "            all_details['db_password'] = info_dict[\"db_password\"]\n",
    "            all_details['db_host'] = info_dict[\"db_host\"]\n",
    "            all_details['db_port'] = info_dict[\"db_port\"]\n",
    "\n",
    "            return all_details\n",
    "        else:\n",
    "            init_logger.info(\"Important Variables not set.\")\n",
    "            init_logger.info(\"Exiting ...\")\n",
    "            sys.exit()\n",
    "    else:\n",
    "        i1 = check_server.chk_srvr()\n",
    "        if (i1.chk_base_dirs() != -1) & (i1.chk_creds() != -1):\n",
    "    \n",
    "            init_logger.info(\"Checking Environment Variables for information\")\n",
    "            srv_dirs = i1.chk_base_dirs()\n",
    "            db_creds = i1.chk_creds()\n",
    "\n",
    "            if srv_dirs:\n",
    "                all_details['file_loc'] = srv_dirs[0]\n",
    "                all_details['log_file'] = srv_dirs[1]\n",
    "                all_details['final_files'] = srv_dirs[2]\n",
    "                all_details['backup'] = srv_dirs[3]\n",
    "                all_details['trigger_file'] = srv_dirs[4]\n",
    "\n",
    "                init_logger.info(\"Variables Set Successfully for File Locations\")\n",
    "            else:\n",
    "                init_logger.info(\"Environment Variables for Files is not set\")\n",
    "                init_logger.info(\"Checking Environment Variables for DB Credentials \")\n",
    "\n",
    "            if db_creds:\n",
    "                all_details['db'] = db_creds[0]\n",
    "                all_details['db_user'] = db_creds[1]\n",
    "                all_details['db_password'] = db_creds[2]\n",
    "                all_details['db_host'] = db_creds[3]\n",
    "                all_details['db_port'] = db_creds[4]\n",
    "                init_logger.info(\"Variables Set successfully for DB Creds \")\n",
    "                return all_details\n",
    "        else:\n",
    "            init_logger.info(\"Environment Variables not set for DB Credentials.\")\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_checks()\n",
    "# engine = create_engine(f'postgresql://db_admin:S3cu12e_123$@192.168.1.8:5432/AnzaBI')\n",
    "# engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consolidated_data_dict(info_file=None):\n",
    "    if os.name == 'posix':\n",
    "        init_logger.info(\"Getting Details from the Environment Variables\")\n",
    "        all_info = init_checks()\n",
    "    elif os.name == 'nt':\n",
    "        init_logger.info(\"Running on Windows\")\n",
    "        loc = info_file\n",
    "        if os.path.exists(loc):\n",
    "            all_info = init_checks(loc)\n",
    "        else:\n",
    "            init_logger.info(f\"File [{loc}] not Found\")\n",
    "            sys.exit()\n",
    "\n",
    "    dict_list = of.categorize_files(all_info[\"file_loc\"],init_logger)\n",
    "    all_files = of.concat_files(dict_list, all_info[\"file_loc\"],init_logger)\n",
    "    return all_files, all_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_filesys(info_file_loc=None):\n",
    "    ret_vals = get_consolidated_data_dict(info_file_loc)\n",
    "    all_files_dict = ret_vals[0]\n",
    "    all_info_dict = ret_vals[1]\n",
    "    final_files = all_info_dict['final_files']\n",
    "    backup = all_info_dict['backup']\n",
    "\n",
    "    init_logger.info(\"*\"*50)\n",
    "    init_logger.info(\"Adding all Entries to main CSV\")\n",
    "    init_logger.info(\"*\"*50)\n",
    "\n",
    "    for file in all_files_dict.keys():\n",
    "        init_logger.info(f\"Converting all Columns to Lowercase for [{file}]\")\n",
    "        all_files_dict[file].columns = [cols.lower() for cols in all_files_dict[file].columns]\n",
    "\n",
    "        fname = final_files+\"/\"+file+\".csv\"\n",
    "        if os.path.exists(fname):\n",
    "            init_logger.info(f'{fname} already exists')\n",
    "            all_files_dict[file].to_csv(fname,header=False,mode=\"a\",index=False)\n",
    "        else:\n",
    "            init_logger.info(f'Creating DF {fname}')\n",
    "            all_files_dict[file].to_csv(fname,mode=\"a\",index=False)\n",
    "    \n",
    "    init_logger.info(\"*\"*50)\n",
    "    init_logger.info(\"Creating Secondary Backup File\")\n",
    "    init_logger.info(\"*\"*50)\n",
    "\n",
    "    for csv_file in os.listdir(final_files):\n",
    "        init_logger.info(f'Processing [{csv_file}]')\n",
    "        if csv_file.endswith('.csv'):\n",
    "            tmp_df = pd.read_csv(final_files+\"/\"+csv_file)\n",
    "            tmp_df['date_added'] = pd.to_datetime(tmp_df['date_added'],format=\"%d-%m-%y\", errors='ignore')\n",
    "            fname = backup+\"/\"+csv_file.split(\".\")[0]+\".parquet.gzip\"\n",
    "            init_logger.info(f'Name of Backup File is [{fname}]')\n",
    "            if os.path.exists(fname):\n",
    "                init_logger.info(f'Backup File already Exists, Removing')\n",
    "                os.remove(fname)\n",
    "            else:\n",
    "                init_logger.info(f'Creating Backup File [{fname}]')\n",
    "                tmp_df.to_parquet(fname,compression = 'gzip')\n",
    "            init_logger.info(\"*\"*50)\n",
    "    return all_files_dict,all_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_db(file_loc=None):\n",
    "    info = write_data_to_filesys(file_loc)\n",
    "    local_all_info = info[1]\n",
    "    local_all_files = info[0]\n",
    "    try:\n",
    "        engine = create_engine(f'postgresql://{local_all_info[\"db_user\"]}:{local_all_info[\"db_password\"]}@{local_all_info[\"db_host\"]}:{local_all_info[\"db_port\"]}/{local_all_info[\"db\"]}',echo=True)\n",
    "    except:\n",
    "        init_logger.info(\"Unable to connect to the Database, Exiting.. \")\n",
    "        sys.exit()\n",
    "    else:\n",
    "        init_logger.info(\"Database Connection Established\")\n",
    "\n",
    "    #final_files = local_all_info['final_files']\n",
    "\n",
    "    for file in local_all_files.keys():\n",
    "        init_logger.info(f\"Now Processing {file}\")\n",
    "        local_all_files[file].columns = [cols.lower() for cols in local_all_files[file].columns]\n",
    "        \n",
    "        # Change the Column namrs for Fees Billed\n",
    "        # Make month to tnx_month and splitamount tp split_amount\n",
    "        if file == \"Fees Billed\":\n",
    "            init_logger.info(\"The Values of Months before change are \")\n",
    "            init_logger.info(local_all_files[file].columns)\n",
    "            local_all_files[file].rename(columns = {'month':'tnx_month', 'splitamount':'split_amount'}, inplace = True)\n",
    "            init_logger.info(\"** Renaming Columns for Fees Billed.csv **\")\n",
    "            init_logger.info(\"The Values of Months after changes are \")\n",
    "            init_logger.info(local_all_files[file].columns)\n",
    "        #fname = final_files+\"/\"+file+\".csv\"\n",
    "        init_logger.debug(f\"Appending all data in Postgresql Server for [{file}]\")\n",
    "\n",
    "        try:            \n",
    "            local_all_files[file].to_sql(SCHEMA_REF[file],con=engine,if_exists='append',index=None)   # Send the data to database\n",
    "        except:\n",
    "            init_logger.error(f\"***** Unable to write to Database for file [{file}]********* \")\n",
    "        else:\n",
    "            continue\n",
    "        init_logger.info(f'Processed File [{file}]')\n",
    "    init_logger.info(f'Moving log files from [{log_vals[1]}] to [{local_all_info[\"log_file\"]}]')\n",
    "    shutil.copy(log_vals[1],local_all_info[\"log_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    info_loc = \"D:\\Offline_Docs\\Anza\\db_creds.csv\"\n",
    "    write_data_to_db(info_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
